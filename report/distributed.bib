
@article{bakhshi_meanfield_2008,
	title = {{MeanField} analysis for the evaluation of gossip protocols},
	abstract = {Gossip protocols are designed to operate in very large, decentralised networks. A node in such a network bases its decision to interact (gossip) with another node on its partial view of the global system. Because of the size of these networks, analysis of gossip protocols is mostly done using simulation, which tend to be expensive in computation time and memory consumption.},
	journaltitle = {International Conference on the Quantitative Evaluation of Systems ({QEST})},
	author = {Bakhshi, Rena and Cloth, Lucia and Fokkink, Wan and Haverkort, Boudewijn R},
	date = {2008},
	langid = {english},
	file = {Bakhshi et al. - MeanField analysis for the evaluation of gossip pr.pdf:/home/sam/Zotero/storage/V2QJVJLR/Bakhshi et al. - MeanField analysis for the evaluation of gossip pr.pdf:application/pdf},
}

@inproceedings{bakhshi_automating_2010,
	location = {Williamsburg, {VA}, {USA}},
	title = {Automating the Mean-Field Method for Large Dynamic Gossip Networks},
	isbn = {978-1-4244-8082-1},
	url = {http://ieeexplore.ieee.org/document/5600381/},
	doi = {10.1109/QEST.2010.38},
	abstract = {We investigate an abstraction method, called meanﬁeld method, for the performance evaluation of dynamic networks with pairwise communication between nodes. It allows us to evaluate systems with very large numbers of nodes, that is, systems of a size where traditional performance evaluation methods fall short.},
	eventtitle = {2010 Seventh International Conference on the Quantitative Evaluation of Systems ({QEST})},
	pages = {241--250},
	booktitle = {2010 Seventh International Conference on the Quantitative Evaluation of Systems},
	publisher = {{IEEE}},
	author = {Bakhshi, Rena and Endrullis, Jorg and Endrullis, Stefan and Fokkink, Wan and Haverkort, Boudewijn},
	urldate = {2023-09-26},
	date = {2010-09},
	langid = {english},
	file = {Bakhshi et al. - 2010 - Automating the Mean-Field Method for Large Dynamic.pdf:/home/sam/Zotero/storage/MYXVWQB4/Bakhshi et al. - 2010 - Automating the Mean-Field Method for Large Dynamic.pdf:application/pdf},
}

@article{ramabaja_bloom_2020,
	title = {The Bloom Tree},
	url = {http://arxiv.org/abs/2002.03057},
	abstract = {We introduce a data structure that allows for efﬁcient (probabilistic) presence proofs and non-probabilistic absence proofs in a bandwidth efﬁcient and secure way. The Bloom tree combines the idea of Bloom ﬁlters with that of Merkle trees. Bloom ﬁlters are used to verify the presence, or absence of elements in a set. In the case of the Bloom tree, we are interested to verify and transmit the presence, or absence of an element in a secure and bandwidth efﬁcient way to another party. Instead of sending the whole Bloom ﬁlter to check for the presence, or absence of an element, the Bloom tree achieves efﬁcient veriﬁcation by using a compact Merkle multiproof.},
	journaltitle = {{ArXiv}},
	author = {Ramabaja, Lum and Avdullahu, Arber},
	urldate = {2023-11-02},
	date = {2020-02-19},
	langid = {english},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
	file = {Ramabaja and Avdullahu - 2020 - The Bloom Tree.pdf:/home/sam/Zotero/storage/JBCCNG4E/Ramabaja and Avdullahu - 2020 - The Bloom Tree.pdf:application/pdf},
}

@article{goncalves_dotteddb_2017,
	title = {{DottedDB}: Anti-Entropy without Merkle Trees, Deletes without Tombstones},
	abstract = {To achieve high availability in the face of network partitions, many distributed databases adopt eventual consistency, allow temporary conﬂicts due to concurrent writes, and use some form of per-key logical clock to detect and resolve such conﬂicts. Furthermore, nodes synchronize periodically to ensure replica convergence in a process called anti-entropy, normally using Merkle Trees. We present the design of {DottedDB}, a Dynamo-like key-value store, which uses a novel nodewide logical clock framework, overcoming three fundamental limitations of the state of the art: (1) minimize the metadata per key necessary to track causality, avoiding its growth even in the face of node churn; (2) correctly and durably delete keys, with no need for tombstones; (3) offer a lightweight antientropy mechanism to converge replicated data, avoiding the need for Merkle Trees. We evaluate {DottedDB} against {MerkleDB}, an otherwise identical database, but using per-key logical clocks and Merkle Trees for anti-entropy, to precisely measure the impact of the novel approach. Results show that: causality metadata per object always converges rapidly to only one id-counter pair; distributed deletes are correctly achieved without global coordination and with constant metadata; divergent nodes are synchronized faster, with less memory-footprint and with less communication overhead than using Merkle Trees.},
	journaltitle = {{IEEE} Reliable Distributed Systems},
	author = {Gonçalves, Ricardo Jorge Tomé and Almeida, Paulo Sérgio and Baquero, Carlos and Fonte, Vitor},
	date = {2017},
	langid = {english},
	file = {Gonçalves et al. - DottedDB Anti-Entropy without Merkle Trees, Delet.pdf:/home/sam/Zotero/storage/LMJ2BXVK/Gonçalves et al. - DottedDB Anti-Entropy without Merkle Trees, Delet.pdf:application/pdf},
}

@article{auvolat_merkle_2019,
	title = {Merkle Search Trees: Efficient State-Based {CRDTs} in Open Networks},
	abstract = {Most recent {CRDT} techniques rely on a causal broadcast primitive to provide guarantees on the delivery of operation deltas. Such a primitive is unfortunately hard to implement efﬁciently in large open networks, whose membership is often difﬁcult to track. As an alternative, we argue in this paper that pure state-based {CRDTs} can be efﬁciently implemented by encoding states as specialized Merkle trees, and that this approach is well suited to open networks where many nodes may join and leave. At the core of our contribution lies a new kind of Merkle tree, called Merkle Search Tree ({MST}), that implements a balanced search tree while maintaining key ordering. This latter property makes it particularly efﬁcient in the case of updates on sets of sequential keys, a common occurrence in many applications. We use this new data structure to implement a distributed event store, and show its efﬁciency in very large systems with low rates of updates. In particular, we show that in some scenarios our approach is able to achieve both a 66\% reduction of bandwidth cost over a vector-clock approach, as well as a 34\% improvement in consistency level. We ﬁnally suggest other uses of our construction for distributed databases in open networks.},
	journaltitle = {{IEEE} Reliable Distributed Systems},
	author = {Auvolat, Alex and Taıani, Francois},
	date = {2019},
	langid = {english},
	file = {Auvolat and Taıani - Merkle Search Trees Efficient State-Based CRDTs i.pdf:/home/sam/Zotero/storage/Y9XTLVMV/Auvolat and Taıani - Merkle Search Trees Efficient State-Based CRDTs i.pdf:application/pdf},
}

@article{byers_fast_2002,
	title = {Fast Approximate Reconciliation of Set Differences},
	author = {Byers, John and Considine, Jeffrey and Mitzenmacher, Michael},
	date = {2002},
	file = {Byers et al. - Fast Approximate Reconciliation of Set Differences.pdf:/home/sam/Zotero/storage/BXH3JSQE/Byers et al. - Fast Approximate Reconciliation of Set Differences.pdf:application/pdf},
}

@article{kleppmann_byzantine_2020,
	title = {Byzantine Eventual Consistency and the Fundamental Limits of Peer-to-Peer Databases},
	url = {http://arxiv.org/abs/2012.00472},
	abstract = {Sybil attacks, in which a large number of adversary-controlled nodes join a network, are a concern for many peer-to-peer database systems, necessitating expensive countermeasures such as proofof-work. However, there is a category of database applications that are, by design, immune to Sybil attacks because they can tolerate arbitrary numbers of Byzantine-faulty nodes. In this paper, we characterize this category of applications using a consistency model we call Byzantine Eventual Consistency ({BEC}). We introduce an algorithm that guarantees {BEC} based on Byzantine causal broadcast, prove its correctness, and demonstrate near-optimal performance in a prototype implementation.},
	journaltitle = {{ArXiv}},
	author = {Kleppmann, Martin and Howard, Heidi},
	urldate = {2023-11-02},
	date = {2020-12-01},
	langid = {english},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Kleppmann and Howard - 2020 - Byzantine Eventual Consistency and the Fundamental.pdf:/home/sam/Zotero/storage/74HAI9A2/Kleppmann and Howard - 2020 - Byzantine Eventual Consistency and the Fundamental.pdf:application/pdf},
}

@report{shapiro_conflict-free_2011,
	title = {Conflict-free Replicated Data Types},
	abstract = {Replicating data under Eventual Consistency ({EC}) allows any replica to accept updates without remote synchronisation. This ensures performance and scalability in largescale distributed systems (e.g., clouds). However, published {EC} approaches are ad-hoc and error-prone. Under a formal Strong Eventual Consistency ({SEC}) model, we study suﬃcient conditions for convergence. A data type that satisﬁes these conditions is called a Conﬂictfree Replicated Data Type ({CRDT}). Replicas of any {CRDT} are guaranteed to converge in a self-stabilising manner, despite any number of failures. This paper formalises two popular approaches (state- and operation-based) and their relevant suﬃcient conditions. We study a number of useful {CRDTs}, such as sets with clean semantics, supporting both add and remove operations, and consider in depth the more complex Graph data type. {CRDT} types can be composed to develop large-scale distributed applications, and have interesting theoretical properties.},
	number = {7687},
	institution = {{INRIA}},
	author = {Shapiro, Marc and Preguiça, Nuno and Baquero, Carlos and Zawirski, Marek},
	date = {2011},
	langid = {english},
	file = {Shapiro et al. - Conflict-free Replicated Data Types.pdf:/home/sam/Zotero/storage/A4X23GHM/Shapiro et al. - Conflict-free Replicated Data Types.pdf:application/pdf},
}

@article{decandia_dynamo_2007,
	title = {Dynamo: Amazon’s Highly Available Key-value Store},
	abstract = {Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems.},
	author = {{DeCandia}, Giuseppe and Hastorun, Deniz and Jampani, Madan and Kakulapati, Gunavardhan and Lakshman, Avinash and Pilchin, Alex and Sivasubramanian, Swaminathan and Vosshall, Peter and Vogels, Werner},
	date = {2007},
	langid = {english},
	file = {DeCandia et al. - Dynamo Amazon’s Highly Available Key-value Store.pdf:/home/sam/Zotero/storage/2Q857EPW/DeCandia et al. - Dynamo Amazon’s Highly Available Key-value Store.pdf:application/pdf},
}

@article{van_renesse_efficient_2008,
	title = {Efficient Reconciliation and Flow Control for Anti-Entropy Protocols},
	abstract = {The paper shows that anti-entropy protocols can process only a limited rate of updates, and proposes and evaluates a new state reconciliation mechanism as well as a ﬂow control scheme for anti-entropy protocols.},
	author = {van Renesse, Robert},
	date = {2008},
	langid = {english},
	file = {Thomas - Dan Dumitriu Valient Gough Amazon.com, Seattle.pdf:/home/sam/Zotero/storage/7VHP5PDJ/Thomas - Dan Dumitriu Valient Gough Amazon.com, Seattle.pdf:application/pdf},
}

@article{almeida_scalable_2016,
	title = {Scalable and Accurate Causality Tracking for Eventually Consistent Stores},
	abstract = {In cloud computing environments, data storage systems often rely on optimistic replication to provide good performance and availability even in the presence of failures or network partitions. In this scenario, it is important to be able to accurately and efﬁciently identify updates executed concurrently. Current approaches to causality tracking in optimistic replication have problems with concurrent updates: they either (1) do not scale, as they require replicas to maintain information that grows linearly with the number of writes or unique clients; (2) lose information about causality, either by removing entries from client-id based version vectors or using server-id based version vectors, which cause false conﬂicts. We propose a new logical clock mechanism and a logical clock framework that together support a traditional key-value store {API}, while capturing causality in an accurate and scalable way, avoiding false conﬂicts. It maintains concise information per data replica, only linear on the number of replica servers, and allows data replicas to be compared and merged linear with the number of replica servers and versions.},
	author = {Almeida, Paulo Sérgio and Baquero, Carlos and Gonçalves, Ricardo and Preguiça, Nuno and Fonte, Victor},
	date = {2016},
	langid = {english},
	file = {Almeida et al. - Scalable and Accurate Causality Tracking for Event.pdf:/home/sam/Zotero/storage/LJ7WGKPW/Almeida et al. - Scalable and Accurate Causality Tracking for Event.pdf:application/pdf},
}

@article{minsky_set_2003,
	title = {Set reconciliation with nearly optimal communication complexity},
	volume = {49},
	abstract = {We consider the problem of efficiently reconciling two similar sets held by different hosts while minimizing the communication complexity, which we call the set reconciliation problem. We describe an approach to set reconciliation based on a polynomial encoding of sets. The resulting protocols exhibit tractable computational complexity and nearly optimal communication complexity when the sets being reconciled are sparse. Also, these protocols can be adapted to work over a broadcast channel, allowing many clients to reconcile with one host based on a single broadcast, even if each client is missing a different subset.},
	number = {9},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Minsky, Y and Trachtenberg, A and Zippel, R},
	date = {2003},
	langid = {english},
	file = {Minsky et al. - 2003 - Set reconciliation with nearly optimal communicati.pdf:/home/sam/Zotero/storage/UGE5NDJ9/Minsky et al. - 2003 - Set reconciliation with nearly optimal communicati.pdf:application/pdf},
}

@inproceedings{aumasson_siphash_2012,
	location = {Berlin, Heidelberg},
	title = {{SipHash}: A Fast Short-Input {PRF}},
	isbn = {978-3-642-34931-7},
	doi = {10.1007/978-3-642-34931-7_28},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{SipHash}},
	abstract = {{SipHash} is a family of pseudorandom functions optimized for short inputs. Target applications include network traffic authentication and hash-table lookups protected against hash-flooding denial-of-service attacks. {SipHash} is simpler than {MACs} based on universal hashing, and faster on short inputs. Compared to dedicated designs for hash-table lookup, {SipHash} has well-defined security goals and competitive performance. For example, {SipHash} processes a 16-byte input with a fresh key in 140 cycles on an {AMD} {FX}-8150 processor, which is much faster than state-of-the-art {MACs}. We propose that hash tables switch to {SipHash} as a hash function.},
	pages = {489--508},
	booktitle = {Progress in Cryptology - {INDOCRYPT} 2012},
	publisher = {Springer},
	author = {Aumasson, Jean-Philippe and Bernstein, Daniel J.},
	editor = {Galbraith, Steven and Nandi, Mridul},
	date = {2012},
	langid = {english},
	keywords = {Full Adder, Hash Table, Message Authentication, Security Goal, Short Message},
	file = {Full Text PDF:/home/sam/Zotero/storage/BKSG4QRV/Aumasson and Bernstein - 2012 - SipHash A Fast Short-Input PRF.pdf:application/pdf},
}

@report{eastlake_3rd_us_2001,
	title = {{US} Secure Hash Algorithm 1 ({SHA}1)},
	url = {https://datatracker.ietf.org/doc/rfc3174},
	abstract = {The purpose of this document is to make the {SHA}-1 (Secure Hash Algorithm 1) hash algorithm conveniently available to the Internet community. This memo provides information for the Internet community.},
	number = {{RFC} 3174},
	institution = {Internet Engineering Task Force},
	type = {Request for Comments},
	author = {Eastlake 3rd, Donald E. and Jones, Paul},
	urldate = {2023-11-19},
	date = {2001-09},
	doi = {10.17487/RFC3174},
	note = {Num Pages: 22},
	file = {Full Text PDF:/home/sam/Zotero/storage/6YZEBBIQ/Eastlake 3rd and Jones - 2001 - US Secure Hash Algorithm 1 (SHA1).pdf:application/pdf},
}

@article{pakdaman_fluid_2010,
	title = {Fluid limit theorems for stochastic hybrid systems with application to neuron models},
	volume = {42},
	issn = {0001-8678, 1475-6064},
	url = {https://www.cambridge.org/core/journals/advances-in-applied-probability/article/fluid-limit-theorems-for-stochastic-hybrid-systems-with-application-to-neuron-models/ACEF2955800C235698243E5BC8ABD0C6},
	doi = {10.1239/aap/1282924062},
	abstract = {In this paper we establish limit theorems for a class of stochastic hybrid systems (continuous deterministic dynamics coupled with jump Markov processes) in the fluid limit (small jumps at high frequency), thus extending known results for jump Markov processes. We prove a functional law of large numbers with exponential convergence speed, derive a diffusion approximation, and establish a functional central limit theorem. We apply these results to neuron models with stochastic ion channels, as the number of channels goes to infinity, estimating the convergence to the deterministic model. In terms of neural coding, we apply our central limit theorems to numerically estimate the impact of channel noise both on frequency and spike timing coding.},
	pages = {761--794},
	number = {3},
	journaltitle = {Advances in Applied Probability},
	author = {Pakdaman, K. and Thieullen, M. and Wainrib, G.},
	urldate = {2023-11-25},
	date = {2010-09},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {60F05, 60F17, 60J75, 92C20, 92C45, fluid limit, Hodgkin-Huxley, kinetic model, Langevin approximation, Morris-Lecar, neuron model, piecewise-deterministic Markov process, Stochastic hybrid system, stochastic ion channels},
	file = {Full Text PDF:/home/sam/Zotero/storage/4I296CXR/Pakdaman et al. - 2010 - Fluid limit theorems for stochastic hybrid systems.pdf:application/pdf},
}

@article{kurtz_limit_1971,
	title = {Limit Theorems for Sequences of Jump Markov Processes Approximating Ordinary Differential Processes},
	volume = {8},
	issn = {0021-9002},
	url = {https://www.jstor.org/stable/3211904},
	doi = {10.2307/3211904},
	pages = {344--356},
	number = {2},
	journaltitle = {Journal of Applied Probability},
	author = {Kurtz, T. G.},
	urldate = {2023-11-25},
	date = {1971},
	file = {JSTOR Full Text PDF:/home/sam/Zotero/storage/MKYKBH2F/Kurtz - 1971 - Limit Theorems for Sequences of Jump Markov Proces.pdf:application/pdf},
}
